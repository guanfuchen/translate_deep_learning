% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
\pdfoutput=1
\documentclass[letterpaper]{article}
\usepackage{CJKutf8}
%\usepackage[UTF8, heading = false, scheme = plain]{ctex}
\usepackage{times}
\usepackage{graphicx}
\usepackage{breakcites}
%\usepackage{authorindex}
\usepackage{algorithm,algorithmic,a4wide,amssymb,natbib,multicol,enumitem,url}
%\usepackage{hyperref}
%\usepackage[hyphenbreaks]{breakurl}
\usepackage[breaklinks]{hyperref}
%\usepackage{algorithm,algorithmic,a4wide,amssymb,natbib,multicol,enumitem,hyperref,url}
% natbib link joining; somewhat breaks \cite, \citet, but is ok for \citep
 \usepackage[top=3.7cm, bottom=3.7cm, left=3.7cm, right=3.7cm]{geometry} 
\makeatletter
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
\begin{document}
\begin{CJK*}{UTF8}{gbsn}

\title{Deep Learning in  Neural Networks: An Overview  \\
{\small Technical Report IDSIA-03-14 / arXiv:1404.7828 v4 [cs.NE] (88 pages, 888 references)}}

\date{8 October 2014}
\author{J\"{u}rgen Schmidhuber~\\
The Swiss AI Lab IDSIA \\
Istituto Dalle Molle di Studi sull'Intelligenza  Artificiale\\
University of Lugano~\& SUPSI \\
Galleria 2, 6928 Manno-Lugano~\\
Switzerland}
\maketitle


\begin{abstract}
In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning.
%triggering a {\em reNNaissance} (title of IJCNN 2011 keynote).
This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their {\em credit assignment paths}, which are chains of possibly learnable, causal links between 
actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.

\vspace{7mm}
\noindent
LATEX source: \url{http://www.idsia.ch/~juergen/DeepLearning8Oct2014.tex} \\
Complete BIBTEX file (888 kB): \url{http://www.idsia.ch/~juergen/deep.bib}

\end{abstract}

%arxiv: In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.


\vspace{1cm}
\subsubsection*{Preface}
\label{foreword}
This is the preprint of an invited {\em Deep Learning} (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using ``local search" to follow citations of citations  backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed, aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and  suggestions to {\em juergen@idsia.ch}.

%{\em Note:} This is the draft of an invited {\em Deep Learning} survey. It mostly  consists of references (over 850 entries so far). Important references are still missing though. As a machine learning researcher, I am obsessed with accurate credit assignment---please do not hesitate to send corrections / improvements / suggestions / additional DL success stories to {\em juergen@idsia.ch}.



\newpage
%\vspace*{-7\baselineskip}
%\vspace*{4\baselineskip}
\tableofcontents

\subsubsection*{Abbreviations in Alphabetical Order}
\label{abb}
\vspace{-0.5cm}
\begin{multicols}{2}
\begin{itemize}[leftmargin=0cm,itemindent=0cm,labelwidth=\itemindent,labelsep=0cm,align=left,noitemsep,nolistsep]
\item[] AE: Autoencoder    
\item[] AI: Artificial Intelligence 
\item[] ANN: Artificial Neural Network    
\item[] BFGS: Broyden-Fletcher-Goldfarb-Shanno    
\item[] BNN: Biological Neural Network 
\item[] BM: Boltzmann Machine 
\item[] BP: Backpropagation 
\item[] BRNN: Bi-directional Recurrent Neural Network 
\item[] CAP: Credit Assignment Path 
\item[] CEC: Constant Error Carousel 
\item[] CFL: Context Free Language 
\item[] CMA-ES: Covariance Matrix Estimation ES 
\item[] CNN: Convolutional Neural Network 
\item[] CoSyNE: Co-Synaptic Neuro-Evolution 
\item[] CSL: Context Senistive Language
\item[] CTC : Connectionist Temporal Classification 
\item[] DBN: Deep Belief Network 
\item[] DCT: Discrete Cosine Transform
\item[] DL: Deep Learning 
\item[] DP: Dynamic Programming  
\item[] DS: Direct Policy Search
\item[] EA: Evolutionary Algorithm 
\item[] EM: Expectation Maximization 
\item[] ES: Evolution Strategy 
\item[] FMS: Flat Minimum Search
\item[] FNN: Feedforward Neural Network  
\item[] FSA: Finite State Automaton 
\item[] GMDH: Group Method of Data Handling  
\item[] GOFAI: Good Old-Fashioned AI  
\item[] GP: Genetic Programming 
\item[] GPU: Graphics Processing Unit 
\item[] GPU-MPCNN: GPU-Based MPCNN  
\item[] HMM: Hidden Markov Model 
\item[] HRL: Hierarchical Reinforcement Learning 
\item[] HTM:  Hierarchical Temporal Memory 
\item[] HMAX:  Hierarchical Model ``and X"
\item[] LSTM: Long Short-Term Memory (RNN) 
\item[] MDL: Minimum Description Length 
\item[] MDP:  Markov Decision Process 
\item[] MNIST: Mixed National Institute of Standards and Technology Database 
\item[] MP: Max-Pooling 
\item[] MPCNN: Max-Pooling CNN
%\item[] NARX: Nonlinear AutoRegressive with eXogenous inputs
\item[] NE: NeuroEvolution 
\item[] NEAT: NE of Augmenting Topologies 
\item[] NES: Natural Evolution Strategies 
\item[] NFQ: Neural Fitted Q-Learning 
\item[] NN: Neural Network 
\item[] OCR: Optical Character Recognition
\item[] PCC: Potential Causal Connection 
\item[] PDCC: Potential Direct Causal Connection 
\item[] PM: Predictability Minimization 
\item[] POMDP:  Partially Observable MDP 
\item[] RAAM: Recursive Auto-Associative Memory  
\item[] RBM: Restricted Boltzmann Machine 
\item[] ReLU: Rectified Linear Unit
\item[] RL: Reinforcement Learning 
\item[] RNN: Recurrent Neural Network 
\item[] R-prop: Resilient Backpropagation 
\item[] SL: Supervised Learning 
\item[] SLIM NN: Self-Delimiting  Neural Network  
\item[] SOTA: Self-Organising Tree Algorithm
\item[] SVM: Support Vector Machine 
\item[] TDNN: Time-Delay Neural Network 
\item[] TIMIT: TI/SRI/MIT Acoustic-Phonetic Continuous Speech Corpus
\item[] UL: Unsupervised Learning 
\item[] WTA:  Winner-Take-All   
\end{itemize}
\end{multicols}
