% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
\pdfoutput=1
\documentclass[letterpaper]{article}
\usepackage{CJKutf8}
%\usepackage[UTF8, heading = false, scheme = plain]{ctex}
\usepackage{times}
\usepackage{graphicx}
\usepackage{breakcites}
%\usepackage{authorindex}
\usepackage{algorithm,algorithmic,a4wide,amssymb,natbib,multicol,enumitem,url}
%\usepackage{hyperref}
%\usepackage[hyphenbreaks]{breakurl}
\usepackage[breaklinks]{hyperref}
%\usepackage{algorithm,algorithmic,a4wide,amssymb,natbib,multicol,enumitem,hyperref,url}
% natbib link joining; somewhat breaks \cite, \citet, but is ok for \citep
 \usepackage[top=3.7cm, bottom=3.7cm, left=3.7cm, right=3.7cm]{geometry} 
\makeatletter
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
\begin{document}
\begin{CJK*}{UTF8}{gbsn}


\section{Introduction to Deep Learning (DL) in Neural Networks (NNs)}
\label{intro}

Which modifiable components of a learning system are responsible for its success or failure?
What changes to them improve performance? 
This has been called the {\em fundamental credit assignment problem}~\citep{Minsky:63}.
There are general credit assignment methods for {\em universal problem solvers} 
that are 
time-optimal in various theoretical senses
%~\citep{Levin:73,Hutter:05book+,Schmidhuber:05gmai} 
(Sec.~\ref{unirl}).
The present survey, however, will focus on the narrower, but now commercially important, subfield
of {\em Deep Learning} (DL) in {\em Artificial Neural Networks} (NNs).


A standard neural network (NN) consists of many simple, connected
processors called neurons, each producing a sequence of real-valued
activations.  Input neurons get activated through sensors perceiving the
environment, other neurons get activated through weighted 
connections from previously active neurons (details in Sec.~\ref{notation}).  
Some neurons may influence the environment
by triggering actions.  {\em Learning} or {\em credit assignment} is
about finding weights that make the NN exhibit {\em desired} behavior,
such as driving a car.  Depending on the problem and how the neurons
are connected, such behavior may require long causal chains of
computational stages (Sec.~\ref{caps}), where each stage transforms
(often in a non-linear way) the aggregate activation of the
network. Deep Learning is about accurately assigning credit across
{\em many} such stages.



{\em Shallow} NN-like models with {\em few} such stages have been around for many decades 
if not centuries  (Sec.~\ref{1940}).
Models with several successive nonlinear layers of  neurons date back at least 
to the 1960s (Sec.~\ref{1965}) and  1970s (Sec.~\ref{1970}).
An efficient gradient descent method for teacher-based {\em Supervised Learning} (SL) in discrete,
differentiable networks 
of arbitrary depth 
called {\em  backpropagation} (BP)
was developed in the 1960s and 1970s, and applied to NNs in 1981 (Sec.~\ref{1970}).  
BP-based 
training of {\em deep} NNs with {\em many} layers, however, 
had been found to be difficult in practice by the late 1980s (Sec.~\ref{1990}), and had
become an explicit research subject 
by the early 1990s (Sec.~\ref{1991a}). 
DL became 
practically feasible to some extent through the help of
{\em Unsupervised Learning} (UL), e.g., Sec.~\ref{1991b} (1991), Sec.~\ref{2006} (2006).
The 1990s and 2000s also saw many improvements of 
purely supervised DL (Sec.~\ref{super}). 
In the new millennium, deep NNs have finally attracted wide-spread attention,
mainly by outperforming alternative machine learning methods
such as kernel machines~\citep{Vapnik:95,advkernel}
in numerous important applications.
In fact, since 2009,
supervised deep NNs
have won many
official international pattern recognition competitions (e.g.,
Sec.~\ref{2009},~\ref{2011},~\ref{2012},~\ref{2013}),  
achieving the first superhuman visual pattern recognition results in
limited domains (Sec.~\ref{2011}, 2011).
Deep NNs also have become relevant for the more general field of 
{\em Reinforcement Learning} (RL) where there is no supervising teacher (Sec.~\ref{deeprl}).


Both {\em feedforward} (acyclic) NNs (FNNs)
and {\em recurrent} (cyclic) NNs (RNNs) have won contests 
(Sec. \ref{1994}, \ref{2003}, \ref{2009}, \ref{2011}, \ref{2012}, \ref{2013}). 
In a sense, RNNs are the deepest of all NNs (Sec.~\ref{caps})---they are general computers
%~\citep{Goedel:31,Church:36,Turing:36,Post:36}
more powerful than FNNs, 
and can in principle create and process
 memories of arbitrary sequences of input patterns~\citep[e.g.,][]{siegelmann91turing,schmidhuber1990}. 
Unlike traditional methods for automatic sequential program synthesis~\citep[e.g.,][]{waldinger69,balzer1985,soloway1986,Deville:94}, RNNs can learn programs that mix sequential and parallel information processing in a natural and efficient way, exploiting the massive parallelism viewed as crucial for sustaining the rapid decline of computation cost observed over the past 75 years. 

The rest of this paper is
structured as follows.
Sec.~\ref{notation} introduces
a compact, event-oriented notation that is simple yet general enough to accommodate both
FNNs and RNNs. 
Sec.~\ref{caps} introduces the concept of {\em Credit Assignment Paths} (CAPs) to measure whether learning in a given  NN application is of the {\em deep} or {\em shallow} type.
Sec.~\ref{themes} lists recurring themes of DL in SL, UL, and RL. 
Sec.~\ref{super}  focuses on SL and UL, 
and on how UL can facilitate SL, although pure SL
has become dominant in recent competitions  
(Sec.~\ref{2009}--\ref{dominant}).
Sec.~\ref{super} is arranged in a
historical timeline format with 
subsections on
important inspirations and technical contributions.
Sec.~\ref{deeprl} on deep RL discusses traditional
{\em Dynamic Programming} (DP)-based RL 
combined with gradient-based search techniques for SL or UL in deep NNs, 
as well as general methods for direct and indirect search in the weight space of deep 
FNNs and RNNs, 
including successful policy gradient and evolutionary methods.

 




\bibliography{deep}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliography{bib,bib_extra}
%\bibliographystyle{alpha}
%\bibliographystyle{apalike}
%\printauthorindex
\end{CJK*}
\end{document}
